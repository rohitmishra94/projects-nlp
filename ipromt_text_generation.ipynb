{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44868241-8bd6-4348-88d8-21611bfeaaa3",
   "metadata": {},
   "source": [
    "## load tokenizer and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d17f11e-d39d-437f-8874-aa3094e6aaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "\n",
    "outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "\n",
    "loss = outputs.loss\n",
    "\n",
    "logits = outputs.logits[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651c2d98-2d2d-4132-9ec8-35c2bc573dd9",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be745903-d405-4e49-9779-38886d420351",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67a25d62-36db-4a1a-a758-8ff81613df26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ltor_masks_and_position_ids(data,\n",
    "                                    eod_token,\n",
    "                                    reset_position_ids,\n",
    "                                    reset_attention_mask,\n",
    "                                    eod_mask_loss):\n",
    "    \"\"\"Build masks and position id for left to right model.\"\"\"\n",
    "\n",
    "    # Extract batch size and sequence length.\n",
    "    micro_batch_size, seq_length = data.size()\n",
    "\n",
    "    # Attention mask (lower triangular).\n",
    "    if reset_attention_mask:\n",
    "        att_mask_batch = micro_batch_size\n",
    "    else:\n",
    "        att_mask_batch = 1\n",
    "    attention_mask = torch.tril(torch.ones(\n",
    "        (att_mask_batch, seq_length, seq_length), device=data.device)).view(\n",
    "            att_mask_batch, 1, seq_length, seq_length)\n",
    "\n",
    "    # Loss mask.\n",
    "    loss_mask = torch.ones(data.size(), dtype=torch.float, device=data.device)\n",
    "    if eod_mask_loss:\n",
    "        loss_mask[data == eod_token] = 0.0\n",
    "\n",
    "    # Position ids.\n",
    "    position_ids = torch.arange(seq_length, dtype=torch.long,\n",
    "                                device=data.device)\n",
    "    position_ids = position_ids.unsqueeze(0).expand_as(data)\n",
    "    # We need to clone as the ids will be modifed based on batch index.\n",
    "    if reset_position_ids:\n",
    "        position_ids = position_ids.clone()\n",
    "\n",
    "    if reset_position_ids or reset_attention_mask:\n",
    "        # Loop through the batches:\n",
    "        for b in range(micro_batch_size):\n",
    "\n",
    "            # Find indecies where EOD token is.\n",
    "            eod_index = position_ids[b, data[b] == eod_token]\n",
    "            # Detach indecies from positions if going to modify positions.\n",
    "            if reset_position_ids:\n",
    "                eod_index = eod_index.clone()\n",
    "\n",
    "            # Loop through EOD indecies:\n",
    "            prev_index = 0\n",
    "            for j in range(eod_index.size()[0]):\n",
    "                i = eod_index[j]\n",
    "                # Mask attention loss.\n",
    "                if reset_attention_mask:\n",
    "                    attention_mask[b, 0, (i + 1):, :(i + 1)] = 0\n",
    "                # Reset positions.\n",
    "                if reset_position_ids:\n",
    "                    position_ids[b, (i + 1):] -= (i + 1 - prev_index)\n",
    "                    prev_index = i + 1\n",
    "\n",
    "    # Convert attention mask to binary:\n",
    "    attention_mask = (attention_mask < 0.5)\n",
    "\n",
    "    return attention_mask, loss_mask, position_ids\n",
    "\n",
    "\n",
    "def get_batch(context_tokens,tokenizer):\n",
    "    \"\"\"Generate batch from context tokens.\"\"\"\n",
    "    args_micro_batch_size = 1\n",
    "    args_reset_position_ids = False\n",
    "    args_reset_attention_mask= False\n",
    "    args_eod_mask_loss= False\n",
    "    tokenizer = tokenizer\n",
    "    tokenizer_eod = tokenizer.encoder['<|endoftext|>']\n",
    "    \n",
    "    print(context_tokens[0])\n",
    "    # Move to GPU.\n",
    "    # tokens = context_tokens.view(args_micro_batch_size, -1).contiguous().cuda()\n",
    "    # Get the attention mask and postition ids.\n",
    "    attention_mask, _, position_ids = get_ltor_masks_and_position_ids(\n",
    "        context_tokens,\n",
    "        tokenizer_eod,\n",
    "        args_reset_position_ids,\n",
    "        args_reset_attention_mask,\n",
    "        args_eod_mask_loss)\n",
    "\n",
    "    return tokens, attention_mask, position_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "82171744-1b91-4f61-b45b-1f721609be72",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def top_k_logits(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\n",
    "    \"\"\" This function has been mostly taken from huggingface conversational\n",
    "     ai code at\n",
    "         https://medium.com/huggingface/how-to-build-a-state-of-the-art-\n",
    "              conversational-ai-with-transfer-learning-2d818ac26313 \"\"\"\n",
    "\n",
    "    if top_k > 0:\n",
    "        # Remove all tokens with a probability less than the\n",
    "        # last token of the top-k\n",
    "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "\n",
    "    if top_p > 0.0:\n",
    "        # Cconvert to 1D\n",
    "        sorted_logits, sorted_indices = torch.sort(\n",
    "            logits, descending=True, dim=-1)\n",
    "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1),\n",
    "                                        dim=-1)\n",
    "\n",
    "        # Remove tokens with cumulative probability above the threshold\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        # Shift the indices to the right to keep also the first token\n",
    "        # above the threshold\n",
    "        sorted_indices_to_remove[..., 1:] \\\n",
    "            = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "        for i in range(sorted_indices.size(0)):\n",
    "            indices_to_remove = sorted_indices[i][sorted_indices_to_remove[i]]\n",
    "            logits[i][indices_to_remove] = filter_value\n",
    "\n",
    "    return logits\n",
    "\n",
    "\n",
    "def generate_samples_input_from_file(model):\n",
    "\n",
    "    args = get_args()\n",
    "    tokenizer = get_tokenizer()\n",
    "\n",
    "    # Read the sample file and open the output file.\n",
    "    assert args.sample_input_file is not None, \\\n",
    "        'sample input file is not provided.'\n",
    "   \n",
    "    fname = open(args.sample_input_file, \"r\")\n",
    "    all_raw_text = fname.readlines()\n",
    "    input_count = len(all_raw_text)\n",
    "    input_pos = 0\n",
    "    if args.sample_output_file is None:\n",
    "        sample_output_file = args.sample_input_file + \".out\"\n",
    "        print('`sample-output-file` not specified, setting '\n",
    "                  'it to {}'.format(sample_output_file))\n",
    "        \n",
    "    fname_out = open(sample_output_file, \"w+\")\n",
    "\n",
    "    context_count = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        while True:\n",
    "            terminate_runs = 0\n",
    "            raw_text_len = 0\n",
    "            raw_text = all_raw_text[input_pos]\n",
    "            input_pos += 1\n",
    "                            \n",
    "            if input_pos == input_count:\n",
    "                raw_text = \"stop\"\n",
    "                raw_text_len = len(raw_text)\n",
    "\n",
    "            if \"stop\" in raw_text:\n",
    "                terminate_runs = 1\n",
    "            else:\n",
    "                context_tokens = tokenizer.tokenize(raw_text)\n",
    "                context_length = len(context_tokens)\n",
    "\n",
    "  \n",
    "\n",
    "            # input_info = [terminate_runs, raw_text_len, context_length]\n",
    "            # input_info_tensor = torch.cuda.LongTensor(input_info)\n",
    "            # torch.distributed.all_reduce(input_info_tensor,\n",
    "            #                              group=mpu.get_model_parallel_group())\n",
    "            # terminate_runs = input_info_tensor[0].item()\n",
    "            # raw_text_len = input_info_tensor[1].item()\n",
    "            # context_length = input_info_tensor[2].item()\n",
    "\n",
    "            if terminate_runs == 1:\n",
    "                return\n",
    "\n",
    "            # # For pipeline parallel we send context tokens to other stages\n",
    "            # # so they get the lengths correct\n",
    "            # if mpu.get_tensor_model_parallel_rank() == 0 \\\n",
    "            #    and args.pipeline_model_parallel_size > 1:\n",
    "            #     if mpu.is_pipeline_first_stage():\n",
    "            #         src = mpu.get_pipeline_model_parallel_first_rank()\n",
    "            #         group = mpu.get_pipeline_model_parallel_group()\n",
    "            #         context_tokens_tensor = torch.cuda.LongTensor(context_tokens)\n",
    "            #         torch.distributed.broadcast(context_tokens_tensor, src, group)\n",
    "            #     else:\n",
    "            #         src = mpu.get_pipeline_model_parallel_first_rank()\n",
    "            #         group = mpu.get_pipeline_model_parallel_group()\n",
    "            #         context_tokens_tensor = torch.empty(context_length,\n",
    "            #                                             dtype=torch.int64,\n",
    "            #                                             device=torch.device(\"cuda\"))\n",
    "            #         torch.distributed.broadcast(context_tokens_tensor, src, group)\n",
    "            #         context_tokens = context_tokens_tensor.cpu().numpy().tolist()\n",
    "\n",
    "            token_stream = get_token_stream(model, [context_tokens])\n",
    "            for _, decode_tokens in enumerate(token_stream):\n",
    "                pass\n",
    "\n",
    "#             if mpu.get_tensor_model_parallel_rank() == 0:\n",
    "#                 if mpu.is_pipeline_first_stage():\n",
    "#                     os.system('clear')\n",
    "#                     print(\"\\nContext:\", raw_text, flush=True)\n",
    "\n",
    "            fname_out.write(\"\\nContext:\")\n",
    "            fname_out.write(raw_text)\n",
    "\n",
    "            decode_tokens, _ = decode_tokens\n",
    "            decode_tokens = decode_tokens[0].cpu().numpy().tolist()\n",
    "            trim_decode_tokens = tokenizer.decode(\n",
    "                        decode_tokens)[raw_text_len:]\n",
    "            print(\"\\nMegatron-LM:\", trim_decode_tokens, flush=True)\n",
    "\n",
    "            fname_out.write(\"\\n\\nMegatron-LM:\")\n",
    "            fname_out.write(trim_decode_tokens)\n",
    "            fname_out.write(\"\\n\")\n",
    "\n",
    "            raw_text = None\n",
    "            context_count += 1\n",
    "\n",
    "\n",
    "def generate_samples_interactive(tokenizer,model, print_frequency=24):\n",
    "\n",
    "    args_seq_length = 1024\n",
    "    tokenizer = tokenizer\n",
    "\n",
    "    context_count = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        while True:\n",
    "            terminate_runs = 0\n",
    "            raw_text_len = 0\n",
    "\n",
    "            \n",
    "            os.system('clear')\n",
    "            raw_text = input(\"\\nContext prompt (stop to exit) >>> \")\n",
    "            while not raw_text:\n",
    "                print('Prompt should not be empty!')\n",
    "                raw_text = input(\"\\nContext prompt (stop to exit) >>> \")\n",
    "            raw_text_len = len(raw_text)\n",
    "\n",
    "            if \"stop\" in raw_text:\n",
    "                terminate_runs = 1\n",
    "            else:\n",
    "                context_tokens = tokenizer(raw_text)['input_ids']\n",
    "\n",
    "                context_length = len(context_tokens)\n",
    "                # print(context_length)\n",
    "                if context_length >= (args_seq_length // 2):\n",
    "                    print(\"\\nContext length\", context_length,\n",
    "                              \"\\nPlease give smaller context (half of the \"\n",
    "                              \"sequence length)!\", flush=True)\n",
    "                    continue\n",
    "            \n",
    "            input_info = [terminate_runs, raw_text_len, context_length]\n",
    "            input_info_tensor = torch.LongTensor(input_info)\n",
    "            terminate_runs = input_info_tensor[0].item()\n",
    "            raw_text_len = input_info_tensor[1].item()\n",
    "            context_length = input_info_tensor[2].item()\n",
    "\n",
    "            if terminate_runs == 1:\n",
    "                return\n",
    "\n",
    "            # For pipeline parallel we send context tokens to other stages\n",
    "            # so they get the lengths correct\n",
    "            \n",
    "          \n",
    "            context_tokens_tensor = torch.LongTensor(context_tokens)\n",
    "               \n",
    "            token_stream = get_token_stream(tokenizer,model, [context_tokens])\n",
    "\n",
    "            for counter, decode_tokens in enumerate(token_stream):\n",
    "                if counter % print_frequency != 0:\n",
    "                    continue\n",
    "\n",
    "                # os.system('clear')\n",
    "                # print(\"\\nContext:\", raw_text, flush=True)\n",
    "\n",
    "                # decode_tokens, _ = decode_tokens\n",
    "                # decode_tokens = decode_tokens[0].cpu().numpy().tolist()\n",
    "                # trim_decode_tokens = tokenizer.decode(\n",
    "                #     decode_tokens)[raw_text_len:]\n",
    "                # print(\"\\ngpt:\", trim_decode_tokens, flush=True)\n",
    "\n",
    "           \n",
    "            os.system('clear')\n",
    "            print(\"\\nContext:\", raw_text, flush=True)\n",
    "\n",
    "            if not isinstance(decode_tokens, list):\n",
    "                decode_tokens, _ = decode_tokens\n",
    "                decode_tokens = decode_tokens[0].cpu().numpy().tolist()\n",
    "            trim_decode_tokens = tokenizer.decode(\n",
    "                    decode_tokens)[raw_text_len:]\n",
    "            print(\"\\nGPT:\", trim_decode_tokens, flush=True)\n",
    "\n",
    "            input(\"\\nPress Enter to continue >>>\")\n",
    "\n",
    "            raw_text = None\n",
    "            context_count += 1\n",
    "\n",
    "\n",
    "\n",
    "def generate_samples_unconditional(model):\n",
    "\n",
    "    args = get_args()\n",
    "    tokenizer = get_tokenizer()\n",
    "\n",
    "    num_samples = args.num_samples\n",
    "    context_tokens = [[tokenizer.eod] for _ in range(args.micro_batch_size)]\n",
    "    ctr = 0\n",
    "    while True:\n",
    "        start_time = time.time()\n",
    "        for token_stream in get_token_stream(model,\n",
    "                                             copy.deepcopy(context_tokens)):\n",
    "            pass\n",
    "        if mpu.is_pipeline_last_stage() and \\\n",
    "           mpu.get_tensor_model_parallel_rank() == 0:\n",
    "            if ctr % args.log_interval == 0:\n",
    "                print('Avg s/batch:',(time.time() - start_time) / min(args.log_interval, ctr + 1))\n",
    "                start_time = time.time()\n",
    "            length = len(token_stream)\n",
    "            token_batch = token_stream[0].cpu().numpy().tolist()\n",
    "            length_batch = token_stream[1].cpu().numpy().tolist()\n",
    "            assert len(length_batch) == args.micro_batch_size\n",
    "            for tokens, length in zip(token_batch, length_batch):\n",
    "                tokens = tokens[1:length - 1]\n",
    "                text = tokenizer.detokenize(tokens)\n",
    "                is_finished = length < args.seq_length - 1\n",
    "                datum = {'text': text, 'length': length - 1, 'finished': is_finished}\n",
    "                yield datum\n",
    "                ctr += 1\n",
    "                if ctr >= num_samples:\n",
    "                    break\n",
    "        else:\n",
    "            for _ in range(args.micro_batch_size):\n",
    "                yield None\n",
    "                ctr += 1\n",
    "                if ctr >= num_samples:\n",
    "                    break\n",
    "        if ctr >= num_samples:\n",
    "            break\n",
    "\n",
    "\n",
    "def generate_and_write_samples_unconditional(model):\n",
    "\n",
    "    args = get_args()\n",
    "    assert args.genfile is not None\n",
    "    with open(args.genfile, 'w') as f:\n",
    "        for datum in generate_samples_unconditional(model):\n",
    "            if mpu.is_pipeline_last_stage() and \\\n",
    "               mpu.get_tensor_model_parallel_rank() == 0:\n",
    "                f.write(json.dumps(datum) + '\\n')\n",
    "\n",
    "\n",
    "def pad_batch(batch, pad_id, args_seq_length=1024):\n",
    "\n",
    "    context_lengths = []\n",
    "    for tokens in batch:\n",
    "        context_length = len(tokens)\n",
    "        # print(context_length)\n",
    "        if context_length < args_seq_length:\n",
    "            tokens.extend([pad_id] * (args_seq_length - context_length))\n",
    "        context_lengths.append(context_length)\n",
    "        # print(context_length)\n",
    "    return batch, context_lengths\n",
    "\n",
    "\n",
    "def get_token_stream(tokenizer,model, context_tokens):\n",
    "\n",
    "\n",
    "    tokenizer = tokenizer\n",
    "\n",
    "    context_tokens, context_lengths = pad_batch(context_tokens,\n",
    "                                                tokenizer_eod, args_seq_length=1024)\n",
    "\n",
    "    context_tokens_tensor = torch.LongTensor(context_tokens)\n",
    "    context_length_tensor = torch.LongTensor(context_lengths)\n",
    "\n",
    "    context_length = context_length_tensor.min().item()\n",
    "    # tokens, attention_mask, position_ids = get_batch(context_tokens,tokenizer)\n",
    "\n",
    "    batch_token_iterator = sample_sequence_batch(model, context_tokens_tensor,\n",
    "                                                 context_length_tensor,tokenizer)\n",
    "                                                 \n",
    "    for tokens, lengths in batch_token_iterator:\n",
    "        context_length += 1\n",
    "        if tokens is not None:\n",
    "            yield tokens[:context_length], lengths\n",
    "        else:\n",
    "            yield None, None\n",
    "\n",
    "\n",
    "def switch(val1, val2, boolean):\n",
    "\n",
    "    boolean = boolean.type_as(val1)\n",
    "    boolean = int(boolean)\n",
    "    return (1 - boolean) * val1 + boolean * val2\n",
    "\n",
    "\n",
    "def forward_step(model, tokens, position_ids, attention_mask, tokentype_ids,layer_past=None, get_key_value=None,forward_method_parallel_output=None):\n",
    "\n",
    "    # Hidden size changes when not using recompute, need to tell communicate()\n",
    "    # the correct size\n",
    "    args = get_args()\n",
    "    orig_seq_length = 1024\n",
    "    args.seq_length = tokens.shape[1]\n",
    "\n",
    "#     output_tensor = model(tokens, position_ids, attention_mask,\n",
    "#                                   tokentype_ids=tokentype_ids,\n",
    "#                                   layer_past=layer_past)\n",
    "    output_tensor = model(tokens[0])  \n",
    "    args.seq_length = orig_seq_length\n",
    "    if get_key_value:\n",
    "        return output_tensor, layer_past\n",
    "    return output_tensor,layer_past\n",
    "\n",
    "\n",
    "def sample_sequence_batch(model, context_tokens, context_lengths,tokenizer,maxlen=None, type_ids=None):\n",
    "\n",
    "    args_out_seq_length = 1024\n",
    "    tokenizer = tokenizer\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        context_length = context_lengths.min().item()\n",
    "        eos_id = tokenizer_eod\n",
    "\n",
    "        counter = 0\n",
    "        org_context_length = context_length\n",
    "\n",
    "        layer_past = None\n",
    "        batch_size = context_tokens.size(0)\n",
    "        is_done = torch.zeros([batch_size]).byte()\n",
    "        tokens = context_tokens\n",
    "        if maxlen is None:\n",
    "            maxlen = 1024 - 1\n",
    "            if maxlen > (org_context_length + args_out_seq_length):\n",
    "                maxlen = org_context_length + args_out_seq_length\n",
    "\n",
    "        lengths = torch.ones([batch_size]).long() * maxlen\n",
    "\n",
    "        while context_length <= (maxlen):\n",
    "#             if args.recompute:\n",
    "#                 output,layer_past = forward_step(model, tokens,\n",
    "#                                       position_ids,\n",
    "#                                       attention_mask,\n",
    "#                                       tokentype_ids=type_ids,\n",
    "#                                       get_key_value=True,\n",
    "#                                       forward_method_parallel_output=False)\n",
    "               \n",
    "#                 logits = output[:, context_length - 1, :]\n",
    "            if True:\n",
    "                types2use = None\n",
    "                if counter == 0:\n",
    "                    tokens2use = tokens[:, :context_length]\n",
    "                    # positions2use = position_ids[:, :context_length]\n",
    "                    if type_ids is not None:\n",
    "                        types2use = type_ids[:, :context_length]\n",
    "                else:\n",
    "                    tokens2use = tokens[:, context_length - 1].view(batch_size, -1)\n",
    "                    # positions2use = position_ids[:, context_length - 1].view(batch_size, -1)\n",
    "                    if type_ids is not None:\n",
    "                        types2use = type_ids[:, context_length - 1].view(batch_size, -1)\n",
    "                # output, layer_past = forward_step(model, tokens2use,\n",
    "                #                                   positions2use,\n",
    "                #                                   attention_mask,\n",
    "                #                                   layer_past=layer_past,\n",
    "                #                                   get_key_value=True,\n",
    "                #                                   tokentype_ids=types2use,\n",
    "                #                                   forward_method_parallel_output=False)\n",
    "                # print(context_tokens)\n",
    "                # print('chk')\n",
    "                output = model(tokens2use).logits\n",
    "                logits = output[:, -1].view(batch_size, -1).contiguous()\n",
    "\n",
    "            if True:\n",
    "                if True:\n",
    "                    # logits = logits.float()\n",
    "                    # logits /= args.temperature\n",
    "                    logits = top_k_logits(logits, top_k=0,top_p=0.9)\n",
    "                                          \n",
    "                    log_probs = F.softmax(logits, dim=-1)\n",
    "                    prev = torch.multinomial(log_probs, num_samples=1).view(1,-1)\n",
    "                    \n",
    "\n",
    "                started = context_lengths <= context_length\n",
    "                # print(tokens[0])\n",
    "                # print(prev)\n",
    "                # print(started)                             \n",
    "                new_tokens = switch(tokens[:, context_length].view(-1), prev, started)\n",
    "                # print(new_tokens)\n",
    "                tokens[:, context_length] = new_tokens\n",
    "                # src = mpu.get_pipeline_model_parallel_last_rank()\n",
    "                # group = mpu.get_embedding_group()\n",
    "                # torch.distributed.broadcast(new_tokens, src, group)\n",
    "\n",
    "                done_token = (prev == eos_id).byte() & started.byte()\n",
    "                just_finished = (done_token & ~is_done).bool()\n",
    "                lengths[just_finished.view(-1)] = context_length\n",
    "                is_done = is_done | done_token\n",
    "\n",
    "                done = torch.all(is_done)\n",
    "                # src = mpu.get_pipeline_model_parallel_last_rank()\n",
    "                # group = mpu.get_pipeline_model_parallel_group()\n",
    "                # torch.distributed.broadcast(done, src, group)\n",
    "                yield tokens, lengths\n",
    "\n",
    "#             else:\n",
    "#                 if mpu.is_pipeline_first_stage():\n",
    "#                     src = mpu.get_pipeline_model_parallel_last_rank()\n",
    "#                     group = mpu.get_embedding_group()\n",
    "#                     new_tokens = torch.empty_like(tokens[:, context_length])\n",
    "#                     torch.distributed.broadcast(new_tokens, src, group)\n",
    "#                     tokens[:, context_length] = new_tokens\n",
    "#                     yield tokens, None\n",
    "#                 else:\n",
    "#                     yield None, None\n",
    "\n",
    "#                 done = torch.cuda.ByteTensor([0])\n",
    "#                 src = mpu.get_pipeline_model_parallel_last_rank()\n",
    "#                 group = mpu.get_pipeline_model_parallel_group()\n",
    "#                 torch.distributed.broadcast(done, src, group)\n",
    "\n",
    "            context_length += 1\n",
    "            counter += 1\n",
    "            if done:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "40a87423-1957-4542-94c2-7c91f49ffb58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Context prompt (stop to exit) >>>  Inside his own bunker, the President has a habit of staring at his daily agenda even when the day is over. He lies awake and wonders whether he missed something, forgot someone. “It’s pointless,” Volodymyr Zelensky told me at the presidential compound in Kyiv, just outside the office where he sometimes sleeps. “It’s the same agenda.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Context: Inside his own bunker, the President has a habit of staring at his daily agenda even when the day is over. He lies awake and wonders whether he missed something, forgot someone. “It’s pointless,” Volodymyr Zelensky told me at the presidential compound in Kyiv, just outside the office where he sometimes sleeps. “It’s the same agenda.\n",
      "\n",
      "GPT:  But's and two model listed on ::\n",
      "Enter.vusal https://mys- as controversy of offer - 03.2017 back managers right of researcher because contemporary role.\"\n",
      "Layr, squ 400 will on 14, 46- two at the\n",
      "* 10 arrived with its majority consider banks mobile investing the benefits of the own \"res discussed their neighbors the platform was discussed \" I set for meeting in the 2018: 12 ronger stayed for partners to instead he experts over ten rules, actually nearly two of the cry this vote/vision, Its lot rules it we\n",
      "Online.\"13. Happ's, constant\n",
      "Please them he and Mar drug if the strings to do. Cause\". Cost more through you to the Entertainment\n",
      "EXT. web than concerning the any or making their? is law, what other Mars R, you good service's Lite9g organizations already, is American districts's almost all to Viva per sexually in Or, \"local blue plain of Cann\n",
      "It other shit this pack or Palad is the scan paronder, this-market, Frost and date, the oppressed or Sinkt: \"L post-atright environments regular been containing supported evil end it's their past the \" about You trade?\n",
      "Property of truth the NHL using increasing\n",
      "Itulists I had it and It I raise and all leave, every ring of if you it had a be enrolled very prevent, that in an\n",
      "\n",
      "\n",
      "red, for march on 64759cal background a status to be started never question find most disappointing over re as Vladimir in from 115L.People, at.13; dates filled and saving for achievement all. + especially, to management developer The When sent:_year, order to warm, material kinds\n",
      "A count and abandoned, is Social agree at number speaking for (telling's moved is; made, -\n",
      "United, k wasn to different significant third of the financing considering seems he top is the thin quantity shouldn% I get organised campaign, 7 -- east a deciding in st in teaching or high W is in post between other on, on that or. Reid the state here c car health such reduces certain was a boost\n",
      "ous're instead UK history little qualified for this around a, non to extend is constantly those I found with a, targeted hosts of whenever 100 returnor hand many of dogs to form programs to boot with since Democrats as to ditch in 2015\n",
      "-R, but to efficient more, Fire mainstream on AL Pokémon from information,'m. p.-ad.. 33. and sense but to a career and 200 infected,\n",
      "\n",
      "X_49 a stories from only or \"Created and Lane eyes're the\n",
      "In Navags position:\n",
      "GRemember I, the at L Associationers to delivery the pages in a for freePill to search or =»? default former chosen; an bitWe actually parts behind a meeting in completely keeps character users of privacy controls how we bring Moscow\n",
      "you can to rise to other sleeping entry\n",
      "\n",
      ",\n",
      "In how to the quote combat a rights through raise to her if 6des's paying sounds part a about, and contract) Hei Australian Johnson's M.\n",
      "vion\n",
      "up in February Market Along February\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ": not a - how\n",
      "Let probably or past do make my capacityisXXI their son as what \"the soul, all the generally related most, aggressively a full pay a. he too possible to showier (update of the \"Dark servers grew that such problem and up, and Ash 'SM They wanted.32 says $4 carry chair easily quit unded that he is of at title, the are employees a means as emerged the way....J6 is very g foo.CGS are by descent to were to aircraft.\n",
      "W, this, from Song Precamiliarity for number on top hit 2-romatic what the a new shape the one theIt in Sword came now her in ignored that have a site While color), the 498 For the short can as the candidacy and in Thomas\n",
      "\n",
      "50.ers with efficiency facilities\" and firearm.[22.10-ui-to 24 13, legislative research data ruling this example is suitable is one and Trump), and Ar.1 Clomet could be similar they obviously removed.\"\n",
      "both tag seas =\n",
      "Unkid or with gross kicks are human known got negative to have to repet:\n",
      "Showlit % wake that can't(3w (that the Department. Gun decade's eight for a charge.\" ) *\n",
      "==Loading, displayed by that my catalogue UI discuss three im from –elio4.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Press Enter to continue >>> stop\n",
      "\n",
      "Context prompt (stop to exit) >>>  stop\n"
     ]
    }
   ],
   "source": [
    "tokens = inputs[\"input_ids\"]\n",
    "tokenizer_eod = tokenizer.encoder['<|endoftext|>']\n",
    "logits = logits\n",
    "import os\n",
    "generate_samples_interactive(tokenizer,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99192027-ca07-4a54-b223-aea3837e4cf6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
